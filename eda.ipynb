{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distribution visualization\n",
    "- Missing value analysis\n",
    "- Correlation analysis with automatic selection of correlation method\n",
    "- Smart normalization based on outlier presence\n",
    "- Outlier visualization\n",
    "- Comprehensive summary report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalEDA:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize the EDA class with a DataFrame\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame to analyze\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "    def plot_distributions(self, figsize=(15, 5*len(self.numeric_columns))):\n",
    "        \"\"\"\n",
    "        Visualize the distribution of numerical variables\n",
    "        - Histogram\n",
    "        - Box plot\n",
    "        - Q-Q plot for normality check\n",
    "        \"\"\"\n",
    "        n_cols = len(self.numeric_columns)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_cols, 3, figsize=figsize)\n",
    "        \n",
    "        for idx, col in enumerate(self.numeric_columns):\n",
    "            # Histogram\n",
    "            sns.histplot(data=self.df, x=col, ax=axes[idx, 0])\n",
    "            axes[idx, 0].set_title(f'{col} Distribution')\n",
    "            \n",
    "            # Box plot\n",
    "            sns.boxplot(data=self.df, y=col, ax=axes[idx, 1])\n",
    "            axes[idx, 1].set_title(f'{col} Boxplot')\n",
    "            \n",
    "            # Q-Q plot\n",
    "            stats.probplot(self.df[col].dropna(), dist=\"norm\", plot=axes[idx, 2])\n",
    "            axes[idx, 2].set_title(f'{col} Q-Q Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def check_missing_values(self):\n",
    "        \"\"\"\n",
    "        Analyze missing values\n",
    "        - Count and percentage of missing values for each variable\n",
    "        - Return rows containing missing values\n",
    "        \"\"\"\n",
    "        missing_count = self.df.isnull().sum()\n",
    "        missing_percent = (missing_count / len(self.df)) * 100\n",
    "        missing_summary = pd.DataFrame({\n",
    "            'Missing Count': missing_count,\n",
    "            'Missing Percent': missing_percent\n",
    "        }).sort_values('Missing Count', ascending=False)\n",
    "        \n",
    "        print(\"\\n=== Missing Value Summary ===\")\n",
    "        print(missing_summary[missing_summary['Missing Count'] > 0])\n",
    "        \n",
    "        print(\"\\n=== Rows with Missing Values ===\")\n",
    "        return self.df[self.df.isnull().any(axis=1)]\n",
    "    \n",
    "    def correlation_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze correlations between variables\n",
    "        - Use Pearson or Spearman correlation based on normality test\n",
    "        \"\"\"\n",
    "        correlation_matrix = pd.DataFrame(index=self.numeric_columns, columns=self.numeric_columns)\n",
    "        method_matrix = pd.DataFrame(index=self.numeric_columns, columns=self.numeric_columns)\n",
    "        \n",
    "        for col1 in self.numeric_columns:\n",
    "            for col2 in self.numeric_columns:\n",
    "                # Shapiro-Wilk test for normality\n",
    "                _, p_val1 = stats.shapiro(self.df[col1].dropna())\n",
    "                _, p_val2 = stats.shapiro(self.df[col2].dropna())\n",
    "                \n",
    "                # If both variables are normally distributed (p > 0.05), use Pearson\n",
    "                if p_val1 > 0.05 and p_val2 > 0.05:\n",
    "                    corr, _ = stats.pearsonr(self.df[col1].dropna(), self.df[col2].dropna())\n",
    "                    method = 'Pearson'\n",
    "                else:\n",
    "                    corr, _ = stats.spearmanr(self.df[col1].dropna(), self.df[col2].dropna())\n",
    "                    method = 'Spearman'\n",
    "                \n",
    "                correlation_matrix.loc[col1, col2] = corr\n",
    "                method_matrix.loc[col1, col2] = method\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n=== Correlation Method Used ===\")\n",
    "        print(method_matrix)\n",
    "        \n",
    "        return correlation_matrix, method_matrix\n",
    "    \n",
    "    def normalize_data(self, columns=None):\n",
    "        \"\"\"\n",
    "        Normalize data using appropriate scaling method\n",
    "        - RobustScaler for data with outliers\n",
    "        - StandardScaler for data without outliers\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        columns : list\n",
    "            List of columns to normalize (None for all numeric variables)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        normalized_df : pandas.DataFrame\n",
    "            Normalized DataFrame\n",
    "        scalers : dict\n",
    "            Dictionary of scaler objects used for each column\n",
    "        \"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.numeric_columns\n",
    "            \n",
    "        normalized_df = self.df.copy()\n",
    "        scalers = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            # Check outliers using IQR method\n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outlier_range = 1.5 * IQR\n",
    "            outliers = ((self.df[col] < (Q1 - outlier_range)) | \n",
    "                       (self.df[col] > (Q3 + outlier_range))).sum()\n",
    "            \n",
    "            # Use RobustScaler if outliers are more than 1% of data\n",
    "            if outliers / len(self.df) >= 0.01:\n",
    "                scaler = RobustScaler()\n",
    "                print(f\"{col}: Using RobustScaler (Found {outliers} outliers)\")\n",
    "            else:\n",
    "                scaler = StandardScaler()\n",
    "                print(f\"{col}: Using StandardScaler\")\n",
    "            \n",
    "            normalized_df[col] = scaler.fit_transform(self.df[[col]])\n",
    "            scalers[col] = scaler\n",
    "        \n",
    "        return normalized_df, scalers\n",
    "    \n",
    "    def plot_outliers(self, columns=None):\n",
    "        \"\"\"\n",
    "        Visualize outliers using box plots\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        columns : list\n",
    "            List of columns to visualize (None for all numeric variables)\n",
    "        \"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.numeric_columns\n",
    "            \n",
    "        n_cols = len(columns)\n",
    "        fig, axes = plt.subplots(n_cols, 1, figsize=(10, 5*n_cols))\n",
    "        if n_cols == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for idx, col in enumerate(columns):\n",
    "            \n",
    "            # Calculate outlier bounds\n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Create boxplot\n",
    "            sns.boxplot(data=self.df, y=col, ax=axes[idx])\n",
    "            \n",
    "            # Add text with outlier information\n",
    "            outliers = ((self.df[col] < lower_bound) | (self.df[col] > upper_bound)).sum()\n",
    "            axes[idx].set_title(f'{col} Outliers: {outliers} points')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive data summary report\n",
    "        - Basic information\n",
    "        - Numeric summary\n",
    "        - Missing values analysis\n",
    "        - Distribution plots\n",
    "        - Correlation analysis\n",
    "        - Outlier visualization\n",
    "        \"\"\"\n",
    "        print(\"=== Data Summary Report ===\")\n",
    "        print(\"\\nBasic Information:\")\n",
    "        print(f\"- Total Rows: {len(self.df)}\")\n",
    "        print(f\"- Total Columns: {len(self.df.columns)}\")\n",
    "        print(f\"- Numeric Columns: {len(self.numeric_columns)}\")\n",
    "        print(f\"- Non-numeric Columns: {len(self.df.columns) - len(self.numeric_columns)}\")\n",
    "        \n",
    "        print(\"\\nNumeric Columns Summary:\")\n",
    "        print(self.df[self.numeric_columns].describe())\n",
    "        \n",
    "        print(\"\\nMissing Values Summary:\")\n",
    "        self.check_missing_values()\n",
    "        \n",
    "        # Generate and save all plots\n",
    "        self.plot_distributions()\n",
    "        plt.savefig('distributions.png')\n",
    "        \n",
    "        self.correlation_analysis()\n",
    "        plt.savefig('correlation.png')\n",
    "        \n",
    "        self.plot_outliers()\n",
    "        plt.savefig('outliers.png')\n",
    "        \n",
    "        print(\"\\nPlots have been saved as:\")\n",
    "        print(\"- distributions.png\")\n",
    "        print(\"- correlation.png\")\n",
    "        print(\"- outliers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example:\n",
    "\"\"\"\n",
    "# Initialize the class with a DataFrame\n",
    "eda = UniversalEDA(df)\n",
    "\n",
    "# Generate comprehensive summary report\n",
    "eda.generate_summary_report()\n",
    "\n",
    "# Or perform specific analyses\n",
    "eda.plot_distributions()  # Check distributions\n",
    "eda.check_missing_values()  # Analyze missing values\n",
    "eda.correlation_analysis()  # Analyze correlations\n",
    "eda.plot_outliers()  # Visualize outliers\n",
    "\n",
    "# Normalize data\n",
    "normalized_df, scalers = eda.normalize_data()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, spearmanr, pearsonr\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# 1. Plot distribution of numerical features\n",
    "def plot_distribution(df):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[col], kde=True, bins=30, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "# 2. Missing Value Analysis\n",
    "def missing_value_analysis(df):\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_counts = missing_counts[missing_counts > 0]\n",
    "    print(\"Missing Values Per Column:\")\n",
    "    print(missing_counts)\n",
    "    \n",
    "    if missing_counts.sum() > 0:\n",
    "        print(\"\\nRows with Missing Values:\")\n",
    "        display(df[df.isnull().any(axis=1)])\n",
    "\n",
    "# 3. Compute Correlation (Pearson if normal, Spearman otherwise)\n",
    "def calculate_correlation(df):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    correlation_matrix = pd.DataFrame(index=num_cols, columns=num_cols)\n",
    "    \n",
    "    for col1 in num_cols:\n",
    "        for col2 in num_cols:\n",
    "            if col1 != col2:\n",
    "                stat, p = shapiro(df[col1].dropna())\n",
    "                if p > 0.05:  # Normally distributed\n",
    "                    corr, _ = pearsonr(df[col1].dropna(), df[col2].dropna())\n",
    "                else:  # Not normally distributed\n",
    "                    corr, _ = spearmanr(df[col1].dropna(), df[col2].dropna())\n",
    "                correlation_matrix.loc[col1, col2] = corr\n",
    "    \n",
    "    correlation_matrix = correlation_matrix.astype(float)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "# 4. Detect Outliers\n",
    "def detect_outliers(df, method=\"zscore\", threshold=3):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    outlier_dict = {}\n",
    "    \n",
    "    for col in num_cols:\n",
    "        if method == \"zscore\":\n",
    "            mean, std = df[col].mean(), df[col].std()\n",
    "            z_scores = (df[col] - mean) / std\n",
    "            outliers = df[np.abs(z_scores) > threshold]\n",
    "        elif method == \"iqr\":\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]\n",
    "        \n",
    "        if not outliers.empty:\n",
    "            outlier_dict[col] = outliers\n",
    "    \n",
    "    return outlier_dict\n",
    "\n",
    "# 5. Normalize Data\n",
    "def normalize_data(df):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    scaler_dict = {}\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    for col in num_cols:\n",
    "        outliers = detect_outliers(df[[col]], method=\"zscore\", threshold=3)\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            scaler = RobustScaler()\n",
    "            print(f\"Using RobustScaler for {col} (outliers detected)\")\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "            print(f\"Using StandardScaler for {col} (no outliers detected)\")\n",
    "        \n",
    "        df_scaled[col] = scaler.fit_transform(df[[col]])\n",
    "        scaler_dict[col] = scaler\n",
    "    \n",
    "    return df_scaled, scaler_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column_name] = df[column_name].fillna(df[column_name].rolling(window, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Time-Based Features:\n",
    "# Basic time features\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day\n",
    "df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "df['week_of_year'] = df['Date'].dt.isocalendar().week\n",
    "df['quarter'] = df['Date'].dt.quarter\n",
    "\n",
    "# Seasonal features\n",
    "df['is_winter'] = df['month'].isin([12, 1, 2])\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8])\n",
    "df['season'] = pd.cut(df['month'], \n",
    "                     bins=[0, 2, 5, 8, 11, 12], \n",
    "                     labels=['Winter', 'Spring', 'Summer', 'Fall', 'Winter'])\n",
    "\n",
    "# Holiday features\n",
    "from holidays import US\n",
    "holidays_us = US()\n",
    "df['is_holiday'] = df['Date'].isin(holidays_us)\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6])\n",
    "\n",
    "###Temperature-Related Features:\n",
    "# Temperature variations\n",
    "df['temp_diff'] = df['actual_temp'] - df['forecasted_average_temp']\n",
    "df['temp_lag_1d'] = df['actual_temp'].shift(1)\n",
    "df['temp_lag_2d'] = df['actual_temp'].shift(2)\n",
    "df['temp_lag_7d'] = df['actual_temp'].shift(7)\n",
    "\n",
    "# Rolling temperature features\n",
    "df['temp_rolling_mean_7d'] = df['actual_temp'].rolling(window=7).mean()\n",
    "df['temp_rolling_std_7d'] = df['actual_temp'].rolling(window=7).std()\n",
    "\n",
    "# Temperature change rates\n",
    "df['temp_change_1d'] = df['actual_temp'] - df['temp_lag_1d']\n",
    "df['temp_change_rate'] = df['temp_change_1d'] / df['temp_lag_1d']\n",
    "\n",
    "\n",
    "###Gas Demand Historical Features:\n",
    "# Lagged demand features\n",
    "df['demand_lag_1d'] = df['actual_gas_demand'].shift(1)\n",
    "df['demand_lag_2d'] = df['actual_gas_demand'].shift(2)\n",
    "df['demand_lag_7d'] = df['actual_gas_demand'].shift(7)\n",
    "df['demand_lag_30d'] = df['actual_gas_demand'].shift(30)\n",
    "\n",
    "# Rolling demand statistics\n",
    "df['demand_rolling_mean_7d'] = df['actual_gas_demand'].rolling(window=7).mean()\n",
    "df['demand_rolling_std_7d'] = df['actual_gas_demand'].rolling(window=7).std()\n",
    "df['demand_rolling_max_7d'] = df['actual_gas_demand'].rolling(window=7).max()\n",
    "\n",
    "\n",
    "###HDD and Wind-Related Features:\n",
    "# HDD features\n",
    "df['hdd_lag_1d'] = df['HDD'].shift(1)\n",
    "df['hdd_rolling_mean_7d'] = df['HDD'].rolling(window=7).mean()\n",
    "\n",
    "# Wind features\n",
    "df['wind_lag_1d'] = df['avg_wind'].shift(1)\n",
    "df['wind_rolling_mean_7d'] = df['avg_wind'].rolling(window=7).mean()\n",
    "\n",
    "# Interaction features\n",
    "df['hdd_wind_interaction'] = df['HDD'] * df['avg_wind']\n",
    "\n",
    "\n",
    "###Forecast Error Features:\n",
    "# Demand forecast error features\n",
    "df['demand_forecast_error'] = df['actual_gas_demand'] - df['forecasted_gas_demand']\n",
    "df['demand_forecast_error_pct'] = df['demand_forecast_error'] / df['forecasted_gas_demand']\n",
    "\n",
    "# Temperature forecast error features\n",
    "df['temp_forecast_error'] = df['actual_temp'] - df['forecasted_average_temp']\n",
    "df['temp_forecast_error_pct'] = df['temp_forecast_error'] / df['forecasted_average_temp']\n",
    "\n",
    "###Cyclical Features (to better capture periodicity):\n",
    "# Cyclical encoding of time features\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
    "\n",
    "\n",
    "###Additional Interaction Features:\n",
    "# Create interaction features between important variables\n",
    "df['temp_hdd_interaction'] = df['actual_temp'] * df['HDD']\n",
    "df['wind_temp_interaction'] = df['avg_wind'] * df['actual_temp']\n",
    "df['month_hdd_interaction'] = df['month'] * df['HDD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important considerations:\n",
    "\n",
    "- Handle missing values created by lag features appropriately\n",
    "- Scale or normalize features if needed (though tree-based models like XGBoost and LightGBM can handle different scales)\n",
    "- Consider removing highly correlated features\n",
    "- Use feature importance from initial models to select most relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill for time series data\n",
    "df = df.fillna(method='ffill')\n",
    "\n",
    "# Or drop rows with missing values if at the start of the dataset\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection using correlation:\n",
    "def remove_highly_correlated_features(df, threshold=0.95, method='pearson'):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df.corr(method=method).abs()\n",
    "    \n",
    "    # Create upper triangle matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features to drop\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    print(f\"Features to drop: {to_drop}\")\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "# 사용 예시:\n",
    "df_features = remove_highly_correlated_features(df, threshold=0.95, method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(df, threshold=0.95, method='pearson'):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features and return both cleaned dataframe and list of removed features\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe\n",
    "    threshold (float): Correlation threshold for removal (default: 0.95)\n",
    "    method (str): Correlation method ('pearson', 'spearman', or 'kendall')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (cleaned_df, dropped_features, correlation_details)\n",
    "        - cleaned_df: DataFrame with highly correlated features removed\n",
    "        - dropped_features: List of removed feature names\n",
    "        - correlation_details: DataFrame containing details of dropped correlations\n",
    "    \"\"\"\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df.corr(method=method).abs()\n",
    "    \n",
    "    # Create upper triangle matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Store correlation details\n",
    "    correlation_details = []\n",
    "    \n",
    "    # Find features to drop\n",
    "    to_drop = []\n",
    "    for column in upper.columns:\n",
    "        # Find highly correlated pairs\n",
    "        high_corr = upper[column][upper[column] > threshold]\n",
    "        \n",
    "        if len(high_corr) > 0:\n",
    "            if column not in to_drop:\n",
    "                to_drop.append(column)\n",
    "                \n",
    "            # Store correlation details\n",
    "            for idx, corr in high_corr.items():\n",
    "                correlation_details.append({\n",
    "                    'dropped_feature': column,\n",
    "                    'correlated_with': idx,\n",
    "                    'correlation': corr\n",
    "                })\n",
    "    \n",
    "    # Convert correlation details to DataFrame\n",
    "    correlation_details = pd.DataFrame(correlation_details)\n",
    "    \n",
    "    # Remove features\n",
    "    cleaned_df = df.drop(columns=to_drop)\n",
    "    \n",
    "    return cleaned_df, to_drop, correlation_details\n",
    "\n",
    "# 사용 예시:\n",
    "cleaned_df, dropped_features, corr_details = remove_highly_correlated_features(df, threshold=0.95, method='pearson')\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\nDropped Features:\", dropped_features)\n",
    "print(\"\\nCorrelation Details:\")\n",
    "print(corr_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_mpae(y_true, y_pred, epsilon=0.5):\n",
    "   \"\"\"\n",
    "   Calculate Mean Percentage Absolute Error with epsilon adjustment for zero values\n",
    "   \"\"\"\n",
    "   y_true = np.array(y_true)\n",
    "   y_pred = np.array(y_pred)\n",
    "   \n",
    "   # Add epsilon to zero values\n",
    "   y_true = np.where(y_true == 0, epsilon, y_true)\n",
    "   \n",
    "   # Calculate percentage absolute error\n",
    "   pae = np.abs((y_true - y_pred) / y_true) * 100\n",
    "   \n",
    "   return np.mean(pae)\n",
    "\n",
    "def train_evaluate_xgboost(X, y, random_state=42):\n",
    "   \"\"\"\n",
    "   Train and evaluate XGBoost model\n",
    "   \"\"\"\n",
    "   # Split data into training and testing sets\n",
    "   X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X, y, test_size=0.2, shuffle=False  # No shuffle for time series data\n",
    "   )\n",
    "   \n",
    "   # Initialize XGBoost model\n",
    "   xgb_model = xgb.XGBRegressor(\n",
    "       n_estimators=1000,\n",
    "       learning_rate=0.01,\n",
    "       max_depth=7,\n",
    "       min_child_weight=1,\n",
    "       subsample=0.8,\n",
    "       colsample_bytree=0.8,\n",
    "       random_state=random_state,\n",
    "       n_jobs=-1\n",
    "   )\n",
    "   \n",
    "   # Train model with early stopping\n",
    "   eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "   xgb_model.fit(\n",
    "       X_train, y_train,\n",
    "       eval_set=eval_set,\n",
    "       eval_metric='rmse',\n",
    "       early_stopping_rounds=50,\n",
    "       verbose=100\n",
    "   )\n",
    "   \n",
    "   # Make predictions\n",
    "   y_pred = xgb_model.predict(X_test)\n",
    "   \n",
    "   # Calculate metrics\n",
    "   metrics = {\n",
    "       'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "       'mae': mean_absolute_error(y_test, y_pred),\n",
    "       'r2': r2_score(y_test, y_pred),\n",
    "       'mpae': calculate_mpae(y_test, y_pred, epsilon=0.5)\n",
    "   }\n",
    "   \n",
    "   # Get feature importance\n",
    "   feature_importance = pd.DataFrame({\n",
    "       'feature': X_train.columns,\n",
    "       'importance': xgb_model.feature_importances_\n",
    "   }).sort_values('importance', ascending=False)\n",
    "   \n",
    "   # Print model performance\n",
    "   print(\"\\n=== Model Performance ===\")\n",
    "   print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "   print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "   print(f\"R2: {metrics['r2']:.4f}\")\n",
    "   print(f\"MPAE: {metrics['mpae']:.4f}%\")\n",
    "   \n",
    "   # Plot feature importance\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   sns.barplot(\n",
    "       data=feature_importance.head(10),\n",
    "       x='importance',\n",
    "       y='feature'\n",
    "   )\n",
    "   plt.title('XGBoost Top 10 Feature Importance')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   # Plot actual vs predicted\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "   plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "   plt.xlabel('Actual')\n",
    "   plt.ylabel('Predicted')\n",
    "   plt.title('XGBoost: Actual vs Predicted')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   results = {\n",
    "       'model': xgb_model,\n",
    "       'predictions': y_pred,\n",
    "       'metrics': metrics,\n",
    "       'feature_importance': feature_importance,\n",
    "       'test_index': X_test.index\n",
    "   }\n",
    "   \n",
    "   return results\n",
    "\n",
    "# 하이퍼파라미터 튜닝을 위한 함수\n",
    "def tune_xgboost(X_train, y_train):\n",
    "   \"\"\"\n",
    "   Tune XGBoost hyperparameters using GridSearchCV\n",
    "   \"\"\"\n",
    "   param_grid = {\n",
    "       'max_depth': [3, 5, 7],\n",
    "       'learning_rate': [0.01, 0.1],\n",
    "       'n_estimators': [100, 500, 1000],\n",
    "       'min_child_weight': [1, 3, 5],\n",
    "       'subsample': [0.6, 0.8, 1.0],\n",
    "       'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "   }\n",
    "   \n",
    "   xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "   grid_search = GridSearchCV(\n",
    "       estimator=xgb_model,\n",
    "       param_grid=param_grid,\n",
    "       cv=TimeSeriesSplit(n_splits=5),\n",
    "       scoring='neg_root_mean_squared_error',\n",
    "       n_jobs=-1,\n",
    "       verbose=2\n",
    "   )\n",
    "   \n",
    "   grid_search.fit(X_train, y_train)\n",
    "   print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "   return grid_search.best_params_\n",
    "\n",
    "# 모델 실행\n",
    "def run_xgboost_modeling(df, target_col, feature_cols):\n",
    "   \"\"\"\n",
    "   Run complete XGBoost modeling process\n",
    "   \"\"\"\n",
    "   # Prepare data\n",
    "   X = df[feature_cols]\n",
    "   y = df[target_col]\n",
    "   \n",
    "   # Train and evaluate model\n",
    "   results = train_evaluate_xgboost(X, y)\n",
    "   \n",
    "   return results\n",
    "\n",
    "# 사용 예시:\n",
    "# feature_cols = [col for col in df.columns if col != target_col]\n",
    "# results = run_xgboost_modeling(df, target_col='actual_gas_demand', feature_cols=feature_cols)\n",
    "\n",
    "# 모델 저장\n",
    "# results['model'].save_model('xgb_model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_mpae(y_true, y_pred, epsilon=0.5):\n",
    "   \"\"\"\n",
    "   Calculate Mean Percentage Absolute Error with epsilon adjustment for zero values\n",
    "   \"\"\"\n",
    "   y_true = np.array(y_true)\n",
    "   y_pred = np.array(y_pred)\n",
    "   \n",
    "   # Add epsilon to zero values\n",
    "   y_true = np.where(y_true == 0, epsilon, y_true)\n",
    "   \n",
    "   # Calculate percentage absolute error\n",
    "   pae = np.abs((y_true - y_pred) / y_true) * 100\n",
    "   \n",
    "   return np.mean(pae)\n",
    "\n",
    "def train_evaluate_lightgbm(X, y, random_state=42):\n",
    "   \"\"\"\n",
    "   Train and evaluate LightGBM model\n",
    "   \"\"\"\n",
    "   # Split data into training and testing sets\n",
    "   X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X, y, test_size=0.2, shuffle=False  # No shuffle for time series data\n",
    "   )\n",
    "   \n",
    "   # Initialize LightGBM model\n",
    "   lgb_model = lgb.LGBMRegressor(\n",
    "       n_estimators=1000,\n",
    "       learning_rate=0.01,\n",
    "       num_leaves=31,\n",
    "       subsample=0.8,\n",
    "       colsample_bytree=0.8,\n",
    "       random_state=random_state,\n",
    "       n_jobs=-1\n",
    "   )\n",
    "   \n",
    "   # Train model with early stopping\n",
    "   eval_set = [(X_test, y_test)]\n",
    "   lgb_model.fit(\n",
    "       X_train, y_train,\n",
    "       eval_set=eval_set,\n",
    "       eval_metric='rmse',\n",
    "       early_stopping_rounds=50,\n",
    "       verbose=100\n",
    "   )\n",
    "   \n",
    "   # Make predictions\n",
    "   y_pred = lgb_model.predict(X_test)\n",
    "   \n",
    "   # Calculate metrics\n",
    "   metrics = {\n",
    "       'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "       'mae': mean_absolute_error(y_test, y_pred),\n",
    "       'r2': r2_score(y_test, y_pred),\n",
    "       'mpae': calculate_mpae(y_test, y_pred, epsilon=0.5)\n",
    "   }\n",
    "   \n",
    "   # Get feature importance\n",
    "   feature_importance = pd.DataFrame({\n",
    "       'feature': X_train.columns,\n",
    "       'importance': lgb_model.feature_importances_\n",
    "   }).sort_values('importance', ascending=False)\n",
    "   \n",
    "   # Print model performance\n",
    "   print(\"\\n=== Model Performance ===\")\n",
    "   print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "   print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "   print(f\"R2: {metrics['r2']:.4f}\")\n",
    "   print(f\"MPAE: {metrics['mpae']:.4f}%\")\n",
    "   \n",
    "   # Plot feature importance\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   sns.barplot(\n",
    "       data=feature_importance.head(10),\n",
    "       x='importance',\n",
    "       y='feature'\n",
    "   )\n",
    "   plt.title('LightGBM Top 10 Feature Importance')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   # Plot actual vs predicted\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "   plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "   plt.xlabel('Actual')\n",
    "   plt.ylabel('Predicted')\n",
    "   plt.title('LightGBM: Actual vs Predicted')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   results = {\n",
    "       'model': lgb_model,\n",
    "       'predictions': y_pred,\n",
    "       'metrics': metrics,\n",
    "       'feature_importance': feature_importance,\n",
    "       'test_index': X_test.index\n",
    "   }\n",
    "   \n",
    "   return results\n",
    "\n",
    "# 하이퍼파라미터 튜닝을 위한 함수\n",
    "def tune_lightgbm(X_train, y_train):\n",
    "   \"\"\"\n",
    "   Tune LightGBM hyperparameters using GridSearchCV\n",
    "   \"\"\"\n",
    "   param_grid = {\n",
    "       'num_leaves': [31, 62, 127],\n",
    "       'learning_rate': [0.01, 0.1],\n",
    "       'n_estimators': [100, 500, 1000],\n",
    "       'subsample': [0.6, 0.8, 1.0],\n",
    "       'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "       'min_child_samples': [20, 50, 100]\n",
    "   }\n",
    "   \n",
    "   lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "   grid_search = GridSearchCV(\n",
    "       estimator=lgb_model,\n",
    "       param_grid=param_grid,\n",
    "       cv=TimeSeriesSplit(n_splits=5),\n",
    "       scoring='neg_root_mean_squared_error',\n",
    "       n_jobs=-1,\n",
    "       verbose=2\n",
    "   )\n",
    "   \n",
    "   grid_search.fit(X_train, y_train)\n",
    "   print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "   return grid_search.best_params_\n",
    "\n",
    "# 모델 실행\n",
    "def run_lightgbm_modeling(df, target_col, feature_cols):\n",
    "   \"\"\"\n",
    "   Run complete LightGBM modeling process\n",
    "   \"\"\"\n",
    "   # Prepare data\n",
    "   X = df[feature_cols]\n",
    "   y = df[target_col]\n",
    "   \n",
    "   # Train and evaluate model\n",
    "   results = train_evaluate_lightgbm(X, y)\n",
    "   \n",
    "   return results\n",
    "\n",
    "# 사용 예시:\n",
    "# feature_cols = [col for col in df.columns if col != target_col]\n",
    "# results = run_lightgbm_modeling(df, target_col='actual_gas_demand', feature_cols=feature_cols)\n",
    "\n",
    "# 모델 저장\n",
    "# results['model'].save_model('lgb_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def train_evaluate_models(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate XGBoost and LightGBM models\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature matrix\n",
    "    y (pd.Series): Target variable\n",
    "    random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing trained models and their performance metrics\n",
    "    \"\"\"\n",
    "    # Time series split for validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False  # No shuffle for time series data\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=7,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train models with early stopping\n",
    "    eval_set_xgb = [(X_train, y_train), (X_test, y_test)]\n",
    "    eval_set_lgb = [(X_test, y_test)]\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=eval_set_xgb,\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Train LightGBM\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=eval_set_lgb,\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    lgb_pred = lgb_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calculate_metrics(y_true, y_pred, model_name):\n",
    "        return {\n",
    "            f'{model_name}_rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            f'{model_name}_mae': mean_absolute_error(y_true, y_pred),\n",
    "            f'{model_name}_r2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    # Get metrics for both models\n",
    "    xgb_metrics = calculate_metrics(y_test, xgb_pred, 'xgb')\n",
    "    lgb_metrics = calculate_metrics(y_test, lgb_pred, 'lgb')\n",
    "    \n",
    "    # Get feature importance\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    lgb_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Combine all results\n",
    "    results = {\n",
    "        'xgb_model': xgb_model,\n",
    "        'lgb_model': lgb_model,\n",
    "        'xgb_predictions': xgb_pred,\n",
    "        'lgb_predictions': lgb_pred,\n",
    "        'metrics': {**xgb_metrics, **lgb_metrics},\n",
    "        'feature_importance': {\n",
    "            'xgb': xgb_importance,\n",
    "            'lgb': lgb_importance\n",
    "        },\n",
    "        'test_index': X_test.index\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 모델 학습 및 평가 실행\n",
    "def run_modeling(df, target_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Run the complete modeling process\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Complete dataset\n",
    "    target_col (str): Name of target column\n",
    "    feature_cols (list): List of feature column names\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = train_evaluate_models(X, y)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== Model Performance ===\")\n",
    "    print(\"\\nXGBoost Metrics:\")\n",
    "    print(f\"RMSE: {results['metrics']['xgb_rmse']:.4f}\")\n",
    "    print(f\"MAE: {results['metrics']['xgb_mae']:.4f}\")\n",
    "    print(f\"R2: {results['metrics']['xgb_r2']:.4f}\")\n",
    "    \n",
    "    print(\"\\nLightGBM Metrics:\")\n",
    "    print(f\"RMSE: {results['metrics']['lgb_rmse']:.4f}\")\n",
    "    print(f\"MAE: {results['metrics']['lgb_mae']:.4f}\")\n",
    "    print(f\"R2: {results['metrics']['lgb_r2']:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(\n",
    "        data=results['feature_importance']['xgb'].head(10),\n",
    "        x='importance',\n",
    "        y='feature'\n",
    "    )\n",
    "    plt.title('XGBoost Top 10 Feature Importance')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(\n",
    "        data=results['feature_importance']['lgb'].head(10),\n",
    "        x='importance',\n",
    "        y='feature'\n",
    "    )\n",
    "    plt.title('LightGBM Top 10 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y[results['test_index']], results['xgb_predictions'], alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('XGBoost: Actual vs Predicted')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y[results['test_index']], results['lgb_predictions'], alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('LightGBM: Actual vs Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 사용 예시:\n",
    "# feature_cols = [col for col in df.columns if col != target_col]\n",
    "# results = run_modeling(df, target_col='actual_gas_demand', feature_cols=feature_cols)\n",
    "\n",
    "# 모델 저장\n",
    "# results['xgb_model'].save_model('xgb_model.json')\n",
    "# results['lgb_model'].save_model('lgb_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_xgboost(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=TimeSeriesSplit(n_splits=5),\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gas_demand_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgas_demand_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 날짜 데이터 변환\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1012\u001b[0m     dialect,\n\u001b[0;32m   1013\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gas_demand_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(\"gas_demand_data.csv\")\n",
    "\n",
    "# 날짜 데이터 변환\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Prophet이 요구하는 데이터 포맷으로 변환\n",
    "prophet_df = df[['Date', 'daily actual gas demand']].rename(columns={'Date': 'ds', 'daily actual gas demand': 'y'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# Prophet 모델 생성 및 학습\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(prophet_df)\n",
    "\n",
    "# 예측할 기간 설정 (6일 후 예측)\n",
    "future = prophet_model.make_future_dataframe(periods=6)\n",
    "forecast = prophet_model.predict(future)\n",
    "\n",
    "# 예측 결과 시각화\n",
    "prophet_model.plot(forecast)\n",
    "plt.show()\n",
    "\n",
    "# 결과 확인\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# 1. 데이터 로드 및 준비\n",
    "data = pd.read_csv('your_dataset.csv')  # 데이터셋 로드\n",
    "data['Date'] = pd.to_datetime(data['Date'])  # 날짜 형식 변환\n",
    "df = data.rename(columns={'Date': 'ds', 'daily actual gas demand': 'y'})  # Prophet 요구 형식\n",
    "\n",
    "# 외부 변수 추가 (예: 날씨 데이터)\n",
    "extra_regressors = ['daily average actual temperature', 'daily average forecasted temperature', \n",
    "                    'HDD', 'daily total temperature', 'day minimum temperature', \n",
    "                    'day maximum temperature', 'daily average wind']\n",
    "\n",
    "# 2. Prophet 모델 설정\n",
    "model = Prophet(daily_seasonality=True, yearly_seasonality=True, weekly_seasonality=True)\n",
    "for regressor in extra_regressors:\n",
    "    model.add_regressor(regressor)\n",
    "\n",
    "# 3. 모델 학습\n",
    "model.fit(df)\n",
    "\n",
    "# 4. 미래 데이터 프레임 생성 (예: 6일 후까지 예측)\n",
    "future_dates = model.make_future_dataframe(periods=6)  # 6일 후까지\n",
    "# 미래 날씨 데이터가 필요하면 외부에서 제공해야 함 (여기서는 과거 데이터 평균으로 임시 대체)\n",
    "for regressor in extra_regressors:\n",
    "    future_dates[regressor] = df[regressor].mean()  # 실제로는 예측된 날씨 데이터를 사용\n",
    "\n",
    "# 5. 예측\n",
    "forecast = model.predict(future_dates)\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(6))  # 예측 결과\n",
    "\n",
    "# 6. 성능 평가 (학습 데이터로 예시)\n",
    "y_true = df['y']\n",
    "y_pred = forecast['yhat'][:len(df)]\n",
    "mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "print(f\"MAPE: {mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "def load_data(file_path):\n",
    "    # 데이터 로드\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 날짜 형식 변환\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. Prophet 모델을 위한 데이터 준비\n",
    "def prepare_prophet_data(df, target_col='daily_actual_gas_demand', date_col='Date'):\n",
    "    # Prophet은 'ds'와 'y' 컬럼명을 사용\n",
    "    prophet_df = df[[date_col, target_col]].rename(columns={\n",
    "        date_col: 'ds',\n",
    "        target_col: 'y'\n",
    "    })\n",
    "    \n",
    "    return prophet_df\n",
    "\n",
    "# 3. 추가 변수(리그레서) 설정\n",
    "def add_regressors(df, prophet_df, regressor_cols):\n",
    "    for col in regressor_cols:\n",
    "        prophet_df[col] = df[col].values\n",
    "    \n",
    "    return prophet_df\n",
    "\n",
    "# 4. Prophet 모델 훈련\n",
    "def train_prophet_model(train_df, regressor_cols=None, forecast_days=1):\n",
    "    # 기본 모델 생성\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.05  # 추세 변화 유연성 조절\n",
    "    )\n",
    "    \n",
    "    # 리그레서 추가\n",
    "    if regressor_cols:\n",
    "        for col in regressor_cols:\n",
    "            model.add_regressor(col)\n",
    "    \n",
    "    # 모델 훈련\n",
    "    model.fit(train_df)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 5. 미래 데이터 예측\n",
    "def make_future_dataframe(model, train_df, forecast_days, regressor_cols=None, future_regressors=None):\n",
    "    # 미래 기간 데이터프레임 생성\n",
    "    future = model.make_future_dataframe(periods=forecast_days)\n",
    "    \n",
    "    # 미래 리그레서 값 추가\n",
    "    if regressor_cols and future_regressors:\n",
    "        for col in regressor_cols:\n",
    "            # 기존 학습 데이터의 리그레서 값 복사\n",
    "            future[col] = train_df[col].values[-len(train_df):]\n",
    "            \n",
    "            # 마지막 forecast_days일의 리그레서 값은 미래 값으로 대체\n",
    "            if col in future_regressors:\n",
    "                future.loc[len(future) - forecast_days:, col] = future_regressors[col]\n",
    "    \n",
    "    return future\n",
    "\n",
    "# 6. 예측 및 평가\n",
    "def predict_and_evaluate(model, future, actual_df, forecast_days):\n",
    "    # 예측 수행\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # 필요한 컬럼만 선택\n",
    "    forecast_result = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "    \n",
    "    # 예측 결과와 실제 값 비교 (테스트 기간에 대해)\n",
    "    if actual_df is not None:\n",
    "        # 예측 결과에서 테스트 기간 데이터만 선택\n",
    "        test_forecast = forecast_result.tail(forecast_days).reset_index(drop=True)\n",
    "        \n",
    "        # 실제 값 준비\n",
    "        test_actual = actual_df.tail(forecast_days).reset_index(drop=True)\n",
    "        \n",
    "        # 평가 지표 계산\n",
    "        mae = mean_absolute_error(test_actual['y'], test_forecast['yhat'])\n",
    "        rmse = np.sqrt(mean_squared_error(test_actual['y'], test_forecast['yhat']))\n",
    "        \n",
    "        print(f\"MAE: {mae:.2f}\")\n",
    "        print(f\"RMSE: {rmse:.2f}\")\n",
    "    \n",
    "    return forecast_result\n",
    "\n",
    "# 7. 결과 시각화\n",
    "def plot_results(model, forecast, actual_df=None):\n",
    "    # Prophet 결과 플롯\n",
    "    fig1 = model.plot(forecast)\n",
    "    \n",
    "    # 구성 요소 플롯 (추세, 계절성 등)\n",
    "    fig2 = model.plot_components(forecast)\n",
    "    \n",
    "    # 실제 값과 예측 값 비교 플롯\n",
    "    if actual_df is not None:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(forecast['ds'].values, forecast['yhat'].values, label='Predicted')\n",
    "        plt.plot(actual_df['ds'].values, actual_df['y'].values, label='Actual')\n",
    "        plt.fill_between(\n",
    "            forecast['ds'].values,\n",
    "            forecast['yhat_lower'].values,\n",
    "            forecast['yhat_upper'].values,\n",
    "            alpha=0.3,\n",
    "            color='gray'\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.title('Actual vs Predicted Gas Demand')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Gas Demand')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return fig1, fig2\n",
    "\n",
    "# 전체 파이프라인\n",
    "def prophet_forecast_pipeline(\n",
    "    file_path, \n",
    "    target_col='daily_actual_gas_demand', \n",
    "    date_col='Date',\n",
    "    regressor_cols=['daily_average_actual_temperature', 'HDD', 'daily_average_wind'],\n",
    "    train_ratio=0.8,\n",
    "    forecast_days=3,\n",
    "    future_regressors=None\n",
    "):\n",
    "    # 데이터 로드\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # 학습/테스트 분할\n",
    "    train_size = int(len(df) * train_ratio)\n",
    "    train_df = df[:train_size]\n",
    "    test_df = df[train_size:]\n",
    "    \n",
    "    # Prophet 데이터 준비\n",
    "    prophet_train_df = prepare_prophet_data(train_df, target_col, date_col)\n",
    "    prophet_test_df = prepare_prophet_data(test_df, target_col, date_col)\n",
    "    \n",
    "    # 리그레서 추가\n",
    "    prophet_train_df = add_regressors(train_df, prophet_train_df, regressor_cols)\n",
    "    \n",
    "    # 모델 훈련\n",
    "    model = train_prophet_model(prophet_train_df, regressor_cols, forecast_days)\n",
    "    \n",
    "    # 미래 데이터프레임 생성\n",
    "    future = make_future_dataframe(model, prophet_train_df, forecast_days, regressor_cols, future_regressors)\n",
    "    \n",
    "    # 예측 및 평가\n",
    "    forecast = predict_and_evaluate(model, future, prophet_test_df, forecast_days)\n",
    "    \n",
    "    # 결과 시각화\n",
    "    plot_results(model, forecast, prophet_test_df)\n",
    "    \n",
    "    return model, forecast\n",
    "\n",
    "# 예시 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 파일 경로 설정\n",
    "    file_path = \"gas_demand_data.csv\"\n",
    "    \n",
    "    # 미래 리그레서 값 (예시)\n",
    "    future_regressors = {\n",
    "        'daily_average_actual_temperature': [10.5, 11.2, 9.8],  # 향후 3일 예상 기온\n",
    "        'HDD': [7.5, 6.8, 8.2],                                 # 향후 3일 예상 HDD\n",
    "        'daily_average_wind': [12.3, 14.1, 11.5]                # 향후 3일 예상 풍속\n",
    "    }\n",
    "    \n",
    "    # 파이프라인 실행\n",
    "    model, forecast = prophet_forecast_pipeline(\n",
    "        file_path=file_path,\n",
    "        forecast_days=3,\n",
    "        future_regressors=future_regressors\n",
    "    )\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"예측 결과:\")\n",
    "    print(forecast.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xLSTM 모델 학습 및 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xLSTM과 기존 LSTM의 주요 차이점은 다음과 같습니다:\n",
    "\n",
    "지수적 메모리 메커니즘(Exponential Memory):\n",
    "\n",
    "xLSTM은 gamma, eta, lambda_param이라는 추가 파라미터를 도입합니다.\n",
    "셀 상태(cell state)를 계산할 때 지수 함수를 사용하여 메모리를 변형합니다: cx_exp = gamma * exp(-eta * |cx|) * cx\n",
    "\n",
    "\n",
    "메모리 제어 파라미터:\n",
    "\n",
    "gamma: 메모리 확장/축소 계수\n",
    "eta: 메모리 변환 강도를 제어\n",
    "lambda_param: 메모리 감쇠 제어 파라미터\n",
    "\n",
    "\n",
    "셀 상태 업데이트 방식:\n",
    "\n",
    "기존 LSTM: cy = forgetgate * cx + ingate * cellgate\n",
    "xLSTM: cy = forgetgate * cx_exp + ingate * cellgate\n",
    "업데이트된 셀 상태에 lambda_param을 곱하여 추가적인 메모리 제어: cy = cy * lambda_param\n",
    "\n",
    "\n",
    "\n",
    "이러한 변화로 인해 xLSTM은 다음과 같은 장점을 가집니다:\n",
    "\n",
    "장기 의존성(long-term dependencies)을 더 효과적으로 처리\n",
    "그래디언트 흐름 개선\n",
    "메모리 제어 메커니즘의 유연성 증가\n",
    "시퀀스 데이터에서 복잡한 패턴을 더 잘 포착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class xLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(xLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Input gate weights\n",
    "        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n",
    "        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n",
    "        \n",
    "        # Additional exponential memory parameters for xLSTM\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.lambda_param = nn.Parameter(torch.ones(hidden_size))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias_ih = nn.Parameter(torch.zeros(4 * hidden_size))\n",
    "            self.bias_hh = nn.Parameter(torch.zeros(4 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "            \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, input, state):\n",
    "        hx, cx = state\n",
    "        \n",
    "        gates = torch.mm(input, self.weight_ih.t())\n",
    "        if self.bias_ih is not None:\n",
    "            gates += self.bias_ih\n",
    "            \n",
    "        gates += torch.mm(hx, self.weight_hh.t())\n",
    "        if self.bias_hh is not None:\n",
    "            gates += self.bias_hh\n",
    "            \n",
    "        # Split gates\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "        \n",
    "        # Apply activations\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "        \n",
    "        # xLSTM cell calculation with exponential memory\n",
    "        cx_exp = self.gamma * torch.exp(-self.eta * torch.abs(cx)) * cx\n",
    "        \n",
    "        # Update cell state with exponential memory component\n",
    "        cy = forgetgate * cx_exp + ingate * cellgate\n",
    "        \n",
    "        # Apply lambda parameter to control memory decay\n",
    "        cy = cy * self.lambda_param\n",
    "        \n",
    "        # Output calculation\n",
    "        hy = outgate * torch.tanh(cy)\n",
    "        \n",
    "        return hy, cy\n",
    "\n",
    "\n",
    "class xLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, \n",
    "                 batch_first=False, dropout=0.0, bidirectional=False):\n",
    "        super(xLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # Create a list of xLSTM cells for each layer and direction\n",
    "        self.cell_list = nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            for direction in range(self.num_directions):\n",
    "                layer_input_size = input_size if layer == 0 else hidden_size * self.num_directions\n",
    "                self.cell_list.append(xLSTMCell(layer_input_size, hidden_size, bias))\n",
    "                \n",
    "        if dropout > 0 and num_layers > 1:\n",
    "            self.dropout_layer = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout_layer = None\n",
    "    \n",
    "    def forward(self, input, hx=None):\n",
    "        \"\"\"\n",
    "        Input: input, (h_0, c_0)\n",
    "            - input: tensor of shape (seq_len, batch, input_size) or (batch, seq_len, input_size)\n",
    "            - h_0: tensor of shape (num_layers * num_directions, batch, hidden_size)\n",
    "            - c_0: tensor of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        Output: output, (h_n, c_n)\n",
    "            - output: tensor of shape (seq_len, batch, hidden_size * num_directions) or (batch, seq_len, hidden_size * num_directions)\n",
    "            - h_n: tensor of shape (num_layers * num_directions, batch, hidden_size)\n",
    "            - c_n: tensor of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        is_batch_first = self.batch_first\n",
    "        if is_batch_first:\n",
    "            input = input.transpose(0, 1)  # Convert to seq_len, batch, features\n",
    "        \n",
    "        seq_len, batch_size, _ = input.size()\n",
    "        \n",
    "        if hx is None:\n",
    "            hx = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size, \n",
    "                            device=input.device)\n",
    "            cx = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size, \n",
    "                            device=input.device)\n",
    "        else:\n",
    "            hx, cx = hx\n",
    "            \n",
    "        # Output tensors for each layer\n",
    "        layer_outputs = []\n",
    "        h_n = []\n",
    "        c_n = []\n",
    "        \n",
    "        # Process input sequence\n",
    "        for layer in range(self.num_layers):\n",
    "            layer_input = input\n",
    "            if layer > 0:\n",
    "                if self.dropout_layer is not None:\n",
    "                    layer_input = self.dropout_layer(layer_input)\n",
    "            \n",
    "            # Forward direction\n",
    "            h_forward = hx[layer*self.num_directions]\n",
    "            c_forward = cx[layer*self.num_directions]\n",
    "            output_forward = []\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                h_forward, c_forward = self.cell_list[layer*self.num_directions](\n",
    "                    layer_input[t], (h_forward, c_forward))\n",
    "                output_forward.append(h_forward)\n",
    "                \n",
    "            output_forward = torch.stack(output_forward)\n",
    "            \n",
    "            # If bidirectional, process in backward direction\n",
    "            if self.bidirectional:\n",
    "                h_backward = hx[layer*self.num_directions + 1]\n",
    "                c_backward = cx[layer*self.num_directions + 1]\n",
    "                output_backward = []\n",
    "                \n",
    "                for t in range(seq_len-1, -1, -1):\n",
    "                    h_backward, c_backward = self.cell_list[layer*self.num_directions + 1](\n",
    "                        layer_input[t], (h_backward, c_backward))\n",
    "                    output_backward.append(h_backward)\n",
    "                    \n",
    "                output_backward = torch.stack(output_backward[::-1])\n",
    "                \n",
    "                # Concatenate forward and backward outputs\n",
    "                layer_output = torch.cat([output_forward, output_backward], dim=2)\n",
    "                \n",
    "                # Add final states to our lists\n",
    "                h_n.append(h_forward)\n",
    "                h_n.append(h_backward)\n",
    "                c_n.append(c_forward)\n",
    "                c_n.append(c_backward)\n",
    "            else:\n",
    "                layer_output = output_forward\n",
    "                h_n.append(h_forward)\n",
    "                c_n.append(c_forward)\n",
    "                \n",
    "            # Set this layer's output as the next layer's input\n",
    "            input = layer_output\n",
    "            layer_outputs.append(layer_output)\n",
    "            \n",
    "        h_n = torch.stack(h_n)\n",
    "        c_n = torch.stack(c_n)\n",
    "        \n",
    "        output = layer_outputs[-1]\n",
    "        if is_batch_first:\n",
    "            output = output.transpose(0, 1)  # Convert back to batch, seq, features\n",
    "            \n",
    "        return output, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. 데이터 로드 및 준비\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "features = ['daily average actual temperature', 'daily average forecasted temperature', \n",
    "            'HDD', 'daily total temperature', 'day minimum temperature', \n",
    "            'day maximum temperature', 'daily average wind', 'daily actual gas demand']\n",
    "df = data[features]\n",
    "\n",
    "# 2. 데이터 정규화\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# 시퀀스 데이터 생성 (과거 7일로 다음 1일 예측)\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :-1])  # 모든 feature 제외 gas demand\n",
    "        y.append(data[i+seq_length, -1])     # gas demand만 타겟\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 7\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.FloatTensor(y)\n",
    "\n",
    "# 3. xLSTM 셀 구현 (sLSTM 기반)\n",
    "class xLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(xLSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 게이트 및 상태 업데이트를 위한 선형 변환\n",
    "        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)  # 입력 게이트\n",
    "        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)  # 망각 게이트\n",
    "        self.W_c = nn.Linear(input_size + hidden_size, hidden_size)  # 셀 상태 후보\n",
    "        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)  # 출력 게이트\n",
    "        self.W_n = nn.Linear(input_size + hidden_size, hidden_size)  # 누적 메모리 가중치 (sLSTM 특유)\n",
    "        \n",
    "    def forward(self, x, h_prev, c_prev, n_prev):\n",
    "        # 입력과 이전 은닉 상태 결합\n",
    "        combined = torch.cat((x, h_prev), dim=1)\n",
    "        \n",
    "        # 게이트 계산\n",
    "        i_t = torch.sigmoid(self.W_i(combined))  # 입력 게이트\n",
    "        f_t = torch.sigmoid(self.W_f(combined))  # 망각 게이트 (지수적 감쇠 반영)\n",
    "        o_t = torch.sigmoid(self.W_o(combined))  # 출력 게이트\n",
    "        c_tilde = torch.tanh(self.W_c(combined)) # 셀 상태 후보\n",
    "        n_tilde = torch.sigmoid(self.W_n(combined)) # 누적 메모리 가중치\n",
    "        \n",
    "        # sLSTM의 셀 상태 업데이트 (정규화된 누적 메모리 반영)\n",
    "        n_t = f_t * n_prev + i_t  # 누적 메모리 (지수 가중 평균 스타일)\n",
    "        c_t = (f_t * c_prev + i_t * c_tilde) / (n_t + 1e-8)  # 정규화된 셀 상태\n",
    "        \n",
    "        # 은닉 상태 업데이트\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t, n_t\n",
    "\n",
    "# xLSTM 모델 정의\n",
    "class xLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(xLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.xlstm_cell = xLSTMCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_size)\n",
    "        n_t = torch.ones(batch_size, self.hidden_size) * 1e-8  # 초기 누적 메모리\n",
    "        \n",
    "        # 시퀀스 처리\n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t, n_t = self.xlstm_cell(x[:, t, :], h_t, c_t, n_t)\n",
    "        \n",
    "        # 마지막 출력으로 예측\n",
    "        out = self.fc(h_t)\n",
    "        return out\n",
    "\n",
    "# 모델 설정\n",
    "input_size = len(features) - 1  # gas demand 제외\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = xLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# 4. 모델 학습\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 5. 예측 (마지막 시퀀스로 1일 후 예측)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    last_sequence = X[-1].unsqueeze(0)  # 마지막 7일 데이터\n",
    "    pred = model(last_sequence)\n",
    "    pred_value = scaler.inverse_transform([[0]*input_size + [pred.item()]])[0][-1]\n",
    "    print(f\"Predicted gas demand: {pred_value}\")\n",
    "\n",
    "# 6. 성능 평가 (별도 테스트셋 필요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xLSTM과 LSTM의 차이점\n",
    "xLSTM은 \"xLSTM: Extended Long Short-Term Memory\" 논문(2024)에서 제안된 모델로, LSTM의 구조를 확장한 것입니다. 주요 차이점은 다음과 같습니다:\n",
    "\n",
    "메모리 구조\n",
    "LSTM: 단일 셀 상태(c_t)를 사용해 정보를 기억하며, 망각 게이트(f_t)로 과거 정보를 지우고 입력 게이트(i_t)로 새 정보를 추가.\n",
    "xLSTM: 두 가지 변형을 제공\n",
    "sLSTM: 스칼라 메모리 기반으로, 누적 메모리 가중치(n_t)를 추가해 셀 상태를 정규화. 위 코드에서 n_t를 통해 이를 반영.\n",
    "mLSTM: 행렬 메모리 기반으로, 더 많은 정보를 병렬적으로 저장(여기서는 간소화로 생략).\n",
    "코드 차이: n_t를 계산하고 셀 상태(c_t)를 n_t로 정규화하는 과정이 추가됨.\n",
    "망각 메커니즘\n",
    "LSTM: 망각 게이트(f_t)는 0~1 사이 값을 곱해 과거 정보를 단순히 스케일링.\n",
    "xLSTM: sLSTM에서는 f_t가 지수 가중 이동 평균(EMA)처럼 작동하며, n_t를 통해 누적된 메모리 크기를 조정. 이로 인해 긴 시퀀스에서 정보 손실이 줄어듦.\n",
    "코드 차이: n_t = f_t * n_prev + i_t로 누적 메모리를 계산.\n",
    "셀 상태 정규화\n",
    "LSTM: 셀 상태(c_t)는 정규화 없이 직접 업데이트(c_t = f_t * c_prev + i_t * c_tilde).\n",
    "xLSTM: 셀 상태를 누적 메모리(n_t)로 나눠 정규화(c_t = (f_t * c_prev + i_t * c_tilde) / n_t). 이로 인해 값의 폭발/소실 방지.\n",
    "코드 차이: c_t 계산 시 / (n_t + 1e-8) 추가.\n",
    "장기 의존성\n",
    "LSTM: 긴 시퀀스에서 기울기 소실 문제로 인해 장기 의존성 학습에 한계.\n",
    "xLSTM: sLSTM과 mLSTM의 조합으로 더 긴 시퀀스에서 패턴을 효과적으로 학습. 특히 sLSTM의 정규화로 안정성 향상.\n",
    "연산 방식\n",
    "LSTM: 단순한 게이트 연산.\n",
    "xLSTM: mLSTM은 행렬 연산을 추가해 병렬 처리 가능(코드에서는 생략). sLSTM은 스칼라 연산으로 효율성과 성능 균형 유지.\n",
    "코드에서 반영된 차이\n",
    "추가 상태(n_t): LSTM에는 없는 누적 메모리 가중치(n_t)를 계산하고, 이를 셀 상태 업데이트에 활용.\n",
    "정규화: c_t를 n_t로 나누는 과정이 LSTM과 다름.\n",
    "구조: xLSTMCell에서 추가적인 선형 변환(W_n)과 계산 로직이 포함됨.\n",
    "참고 사항\n",
    "간소화: 위 코드는 sLSTM 스타일만 반영했으며, mLSTM(행렬 메모리)까지 포함하려면 더 복잡한 구현 필요.\n",
    "미래 예측: 3일, 6일 후 예측을 위해서는 last_sequence에 미래 날씨 데이터를 추가해야 함.\n",
    "성능 최적화: seq_length, hidden_size, 학습률 조정 추천."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ✅ xLSTM 모델 정의\n",
    "class xLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super(xLSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 기존 LSTM과 다른 xLSTM의 핵심: 입력과 망각 게이트를 하나로 합친 구조\n",
    "        self.xlstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                             batch_first=True, dropout=dropout, bidirectional=True)\n",
    "\n",
    "        # 망각 게이트를 개선한 xLSTM 구조\n",
    "        self.forget_gate = nn.Linear(hidden_size * 2, hidden_size * 2)  # Bidirectional\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # 최종 출력 레이어\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # Bidirectional이므로 hidden_size * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # Bidirectional이므로 *2\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # xLSTM 통과\n",
    "        lstm_out, _ = self.xlstm(x, (h0, c0))\n",
    "\n",
    "        # 망각 게이트 개선\n",
    "        forget_vector = self.sigmoid(self.forget_gate(lstm_out[:, -1, :]))\n",
    "\n",
    "        # 최종 출력 계산\n",
    "        output = self.fc(forget_vector)\n",
    "        return output\n",
    "\n",
    "# ✅ 하이퍼파라미터 설정\n",
    "input_size = 7  # Feature 개수\n",
    "hidden_size = 128\n",
    "num_layers = 3  # 깊은 xLSTM 구조 적용\n",
    "dropout = 0.3\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "\n",
    "# ✅ 모델 초기화\n",
    "model = xLSTM(input_size, hidden_size, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ✅ 모델 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(\"cuda\"), y_batch.to(\"cuda\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# ✅ 테스트 평가\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor.to(\"cuda\")).squeeze()\n",
    "    test_loss = criterion(y_pred, y_test_tensor.to(\"cuda\"))\n",
    "    print(f'Test Loss: {test_loss.item()}')\n",
    "\n",
    "# ✅ 결과 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y_test_tensor.cpu().numpy(), label=\"Actual\")\n",
    "plt.plot(y_pred.cpu().numpy(), label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
