{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distribution visualization\n",
    "- Missing value analysis\n",
    "- Correlation analysis with automatic selection of correlation method\n",
    "- Smart normalization based on outlier presence\n",
    "- Outlier visualization\n",
    "- Comprehensive summary report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalEDA:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initialize the EDA class with a DataFrame\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame to analyze\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "    def plot_distributions(self, figsize=(15, 5*len(self.numeric_columns))):\n",
    "        \"\"\"\n",
    "        Visualize the distribution of numerical variables\n",
    "        - Histogram\n",
    "        - Box plot\n",
    "        - Q-Q plot for normality check\n",
    "        \"\"\"\n",
    "        n_cols = len(self.numeric_columns)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_cols, 3, figsize=figsize)\n",
    "        \n",
    "        for idx, col in enumerate(self.numeric_columns):\n",
    "            # Histogram\n",
    "            sns.histplot(data=self.df, x=col, ax=axes[idx, 0])\n",
    "            axes[idx, 0].set_title(f'{col} Distribution')\n",
    "            \n",
    "            # Box plot\n",
    "            sns.boxplot(data=self.df, y=col, ax=axes[idx, 1])\n",
    "            axes[idx, 1].set_title(f'{col} Boxplot')\n",
    "            \n",
    "            # Q-Q plot\n",
    "            stats.probplot(self.df[col].dropna(), dist=\"norm\", plot=axes[idx, 2])\n",
    "            axes[idx, 2].set_title(f'{col} Q-Q Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def check_missing_values(self):\n",
    "        \"\"\"\n",
    "        Analyze missing values\n",
    "        - Count and percentage of missing values for each variable\n",
    "        - Return rows containing missing values\n",
    "        \"\"\"\n",
    "        missing_count = self.df.isnull().sum()\n",
    "        missing_percent = (missing_count / len(self.df)) * 100\n",
    "        missing_summary = pd.DataFrame({\n",
    "            'Missing Count': missing_count,\n",
    "            'Missing Percent': missing_percent\n",
    "        }).sort_values('Missing Count', ascending=False)\n",
    "        \n",
    "        print(\"\\n=== Missing Value Summary ===\")\n",
    "        print(missing_summary[missing_summary['Missing Count'] > 0])\n",
    "        \n",
    "        print(\"\\n=== Rows with Missing Values ===\")\n",
    "        return self.df[self.df.isnull().any(axis=1)]\n",
    "    \n",
    "    def correlation_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze correlations between variables\n",
    "        - Use Pearson or Spearman correlation based on normality test\n",
    "        \"\"\"\n",
    "        correlation_matrix = pd.DataFrame(index=self.numeric_columns, columns=self.numeric_columns)\n",
    "        method_matrix = pd.DataFrame(index=self.numeric_columns, columns=self.numeric_columns)\n",
    "        \n",
    "        for col1 in self.numeric_columns:\n",
    "            for col2 in self.numeric_columns:\n",
    "                # Shapiro-Wilk test for normality\n",
    "                _, p_val1 = stats.shapiro(self.df[col1].dropna())\n",
    "                _, p_val2 = stats.shapiro(self.df[col2].dropna())\n",
    "                \n",
    "                # If both variables are normally distributed (p > 0.05), use Pearson\n",
    "                if p_val1 > 0.05 and p_val2 > 0.05:\n",
    "                    corr, _ = stats.pearsonr(self.df[col1].dropna(), self.df[col2].dropna())\n",
    "                    method = 'Pearson'\n",
    "                else:\n",
    "                    corr, _ = stats.spearmanr(self.df[col1].dropna(), self.df[col2].dropna())\n",
    "                    method = 'Spearman'\n",
    "                \n",
    "                correlation_matrix.loc[col1, col2] = corr\n",
    "                method_matrix.loc[col1, col2] = method\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n=== Correlation Method Used ===\")\n",
    "        print(method_matrix)\n",
    "        \n",
    "        return correlation_matrix, method_matrix\n",
    "    \n",
    "    def normalize_data(self, columns=None):\n",
    "        \"\"\"\n",
    "        Normalize data using appropriate scaling method\n",
    "        - RobustScaler for data with outliers\n",
    "        - StandardScaler for data without outliers\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        columns : list\n",
    "            List of columns to normalize (None for all numeric variables)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        normalized_df : pandas.DataFrame\n",
    "            Normalized DataFrame\n",
    "        scalers : dict\n",
    "            Dictionary of scaler objects used for each column\n",
    "        \"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.numeric_columns\n",
    "            \n",
    "        normalized_df = self.df.copy()\n",
    "        scalers = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            # Check outliers using IQR method\n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outlier_range = 1.5 * IQR\n",
    "            outliers = ((self.df[col] < (Q1 - outlier_range)) | \n",
    "                       (self.df[col] > (Q3 + outlier_range))).sum()\n",
    "            \n",
    "            # Use RobustScaler if outliers are more than 1% of data\n",
    "            if outliers / len(self.df) >= 0.01:\n",
    "                scaler = RobustScaler()\n",
    "                print(f\"{col}: Using RobustScaler (Found {outliers} outliers)\")\n",
    "            else:\n",
    "                scaler = StandardScaler()\n",
    "                print(f\"{col}: Using StandardScaler\")\n",
    "            \n",
    "            normalized_df[col] = scaler.fit_transform(self.df[[col]])\n",
    "            scalers[col] = scaler\n",
    "        \n",
    "        return normalized_df, scalers\n",
    "    \n",
    "    def plot_outliers(self, columns=None):\n",
    "        \"\"\"\n",
    "        Visualize outliers using box plots\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        columns : list\n",
    "            List of columns to visualize (None for all numeric variables)\n",
    "        \"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.numeric_columns\n",
    "            \n",
    "        n_cols = len(columns)\n",
    "        fig, axes = plt.subplots(n_cols, 1, figsize=(10, 5*n_cols))\n",
    "        if n_cols == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for idx, col in enumerate(columns):\n",
    "            \n",
    "            # Calculate outlier bounds\n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Create boxplot\n",
    "            sns.boxplot(data=self.df, y=col, ax=axes[idx])\n",
    "            \n",
    "            # Add text with outlier information\n",
    "            outliers = ((self.df[col] < lower_bound) | (self.df[col] > upper_bound)).sum()\n",
    "            axes[idx].set_title(f'{col} Outliers: {outliers} points')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive data summary report\n",
    "        - Basic information\n",
    "        - Numeric summary\n",
    "        - Missing values analysis\n",
    "        - Distribution plots\n",
    "        - Correlation analysis\n",
    "        - Outlier visualization\n",
    "        \"\"\"\n",
    "        print(\"=== Data Summary Report ===\")\n",
    "        print(\"\\nBasic Information:\")\n",
    "        print(f\"- Total Rows: {len(self.df)}\")\n",
    "        print(f\"- Total Columns: {len(self.df.columns)}\")\n",
    "        print(f\"- Numeric Columns: {len(self.numeric_columns)}\")\n",
    "        print(f\"- Non-numeric Columns: {len(self.df.columns) - len(self.numeric_columns)}\")\n",
    "        \n",
    "        print(\"\\nNumeric Columns Summary:\")\n",
    "        print(self.df[self.numeric_columns].describe())\n",
    "        \n",
    "        print(\"\\nMissing Values Summary:\")\n",
    "        self.check_missing_values()\n",
    "        \n",
    "        # Generate and save all plots\n",
    "        self.plot_distributions()\n",
    "        plt.savefig('distributions.png')\n",
    "        \n",
    "        self.correlation_analysis()\n",
    "        plt.savefig('correlation.png')\n",
    "        \n",
    "        self.plot_outliers()\n",
    "        plt.savefig('outliers.png')\n",
    "        \n",
    "        print(\"\\nPlots have been saved as:\")\n",
    "        print(\"- distributions.png\")\n",
    "        print(\"- correlation.png\")\n",
    "        print(\"- outliers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example:\n",
    "\"\"\"\n",
    "# Initialize the class with a DataFrame\n",
    "eda = UniversalEDA(df)\n",
    "\n",
    "# Generate comprehensive summary report\n",
    "eda.generate_summary_report()\n",
    "\n",
    "# Or perform specific analyses\n",
    "eda.plot_distributions()  # Check distributions\n",
    "eda.check_missing_values()  # Analyze missing values\n",
    "eda.correlation_analysis()  # Analyze correlations\n",
    "eda.plot_outliers()  # Visualize outliers\n",
    "\n",
    "# Normalize data\n",
    "normalized_df, scalers = eda.normalize_data()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, spearmanr, pearsonr\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# 1. Plot distribution of numerical features\n",
    "def plot_distribution(df):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(df[col], kde=True, bins=30, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "# 2. Missing Value Analysis\n",
    "def missing_value_analysis(df):\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_counts = missing_counts[missing_counts > 0]\n",
    "    print(\"Missing Values Per Column:\")\n",
    "    print(missing_counts)\n",
    "    \n",
    "    if missing_counts.sum() > 0:\n",
    "        print(\"\\nRows with Missing Values:\")\n",
    "        display(df[df.isnull().any(axis=1)])\n",
    "\n",
    "# 3. Compute Correlation (Pearson if normal, Spearman otherwise)\n",
    "def calculate_correlation(df):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    correlation_matrix = pd.DataFrame(index=num_cols, columns=num_cols)\n",
    "    \n",
    "    for col1 in num_cols:\n",
    "        for col2 in num_cols:\n",
    "            if col1 != col2:\n",
    "                stat, p = shapiro(df[col1].dropna())\n",
    "                if p > 0.05:  # Normally distributed\n",
    "                    corr, _ = pearsonr(df[col1].dropna(), df[col2].dropna())\n",
    "                else:  # Not normally distributed\n",
    "                    corr, _ = spearmanr(df[col1].dropna(), df[col2].dropna())\n",
    "                correlation_matrix.loc[col1, col2] = corr\n",
    "    \n",
    "    correlation_matrix = correlation_matrix.astype(float)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "# 4. Detect Outliers\n",
    "def detect_outliers(df, method=\"zscore\", threshold=3):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    outlier_dict = {}\n",
    "    \n",
    "    for col in num_cols:\n",
    "        if method == \"zscore\":\n",
    "            mean, std = df[col].mean(), df[col].std()\n",
    "            z_scores = (df[col] - mean) / std\n",
    "            outliers = df[np.abs(z_scores) > threshold]\n",
    "        elif method == \"iqr\":\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]\n",
    "        \n",
    "        if not outliers.empty:\n",
    "            outlier_dict[col] = outliers\n",
    "    \n",
    "    return outlier_dict\n",
    "\n",
    "# 5. Normalize Data\n",
    "def normalize_data(df):\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    scaler_dict = {}\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    for col in num_cols:\n",
    "        outliers = detect_outliers(df[[col]], method=\"zscore\", threshold=3)\n",
    "        \n",
    "        if len(outliers) > 0:\n",
    "            scaler = RobustScaler()\n",
    "            print(f\"Using RobustScaler for {col} (outliers detected)\")\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "            print(f\"Using StandardScaler for {col} (no outliers detected)\")\n",
    "        \n",
    "        df_scaled[col] = scaler.fit_transform(df[[col]])\n",
    "        scaler_dict[col] = scaler\n",
    "    \n",
    "    return df_scaled, scaler_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[column_name] = df[column_name].fillna(df[column_name].rolling(window, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Time-Based Features:\n",
    "# Basic time features\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day\n",
    "df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "df['week_of_year'] = df['Date'].dt.isocalendar().week\n",
    "df['quarter'] = df['Date'].dt.quarter\n",
    "\n",
    "# Seasonal features\n",
    "df['is_winter'] = df['month'].isin([12, 1, 2])\n",
    "df['is_summer'] = df['month'].isin([6, 7, 8])\n",
    "df['season'] = pd.cut(df['month'], \n",
    "                     bins=[0, 2, 5, 8, 11, 12], \n",
    "                     labels=['Winter', 'Spring', 'Summer', 'Fall', 'Winter'])\n",
    "\n",
    "# Holiday features\n",
    "from holidays import US\n",
    "holidays_us = US()\n",
    "df['is_holiday'] = df['Date'].isin(holidays_us)\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6])\n",
    "\n",
    "###Temperature-Related Features:\n",
    "# Temperature variations\n",
    "df['temp_diff'] = df['actual_temp'] - df['forecasted_average_temp']\n",
    "df['temp_lag_1d'] = df['actual_temp'].shift(1)\n",
    "df['temp_lag_2d'] = df['actual_temp'].shift(2)\n",
    "df['temp_lag_7d'] = df['actual_temp'].shift(7)\n",
    "\n",
    "# Rolling temperature features\n",
    "df['temp_rolling_mean_7d'] = df['actual_temp'].rolling(window=7).mean()\n",
    "df['temp_rolling_std_7d'] = df['actual_temp'].rolling(window=7).std()\n",
    "\n",
    "# Temperature change rates\n",
    "df['temp_change_1d'] = df['actual_temp'] - df['temp_lag_1d']\n",
    "df['temp_change_rate'] = df['temp_change_1d'] / df['temp_lag_1d']\n",
    "\n",
    "\n",
    "###Gas Demand Historical Features:\n",
    "# Lagged demand features\n",
    "df['demand_lag_1d'] = df['actual_gas_demand'].shift(1)\n",
    "df['demand_lag_2d'] = df['actual_gas_demand'].shift(2)\n",
    "df['demand_lag_7d'] = df['actual_gas_demand'].shift(7)\n",
    "df['demand_lag_30d'] = df['actual_gas_demand'].shift(30)\n",
    "\n",
    "# Rolling demand statistics\n",
    "df['demand_rolling_mean_7d'] = df['actual_gas_demand'].rolling(window=7).mean()\n",
    "df['demand_rolling_std_7d'] = df['actual_gas_demand'].rolling(window=7).std()\n",
    "df['demand_rolling_max_7d'] = df['actual_gas_demand'].rolling(window=7).max()\n",
    "\n",
    "\n",
    "###HDD and Wind-Related Features:\n",
    "# HDD features\n",
    "df['hdd_lag_1d'] = df['HDD'].shift(1)\n",
    "df['hdd_rolling_mean_7d'] = df['HDD'].rolling(window=7).mean()\n",
    "\n",
    "# Wind features\n",
    "df['wind_lag_1d'] = df['avg_wind'].shift(1)\n",
    "df['wind_rolling_mean_7d'] = df['avg_wind'].rolling(window=7).mean()\n",
    "\n",
    "# Interaction features\n",
    "df['hdd_wind_interaction'] = df['HDD'] * df['avg_wind']\n",
    "\n",
    "\n",
    "###Forecast Error Features:\n",
    "# Demand forecast error features\n",
    "df['demand_forecast_error'] = df['actual_gas_demand'] - df['forecasted_gas_demand']\n",
    "df['demand_forecast_error_pct'] = df['demand_forecast_error'] / df['forecasted_gas_demand']\n",
    "\n",
    "# Temperature forecast error features\n",
    "df['temp_forecast_error'] = df['actual_temp'] - df['forecasted_average_temp']\n",
    "df['temp_forecast_error_pct'] = df['temp_forecast_error'] / df['forecasted_average_temp']\n",
    "\n",
    "###Cyclical Features (to better capture periodicity):\n",
    "# Cyclical encoding of time features\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
    "\n",
    "\n",
    "###Additional Interaction Features:\n",
    "# Create interaction features between important variables\n",
    "df['temp_hdd_interaction'] = df['actual_temp'] * df['HDD']\n",
    "df['wind_temp_interaction'] = df['avg_wind'] * df['actual_temp']\n",
    "df['month_hdd_interaction'] = df['month'] * df['HDD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important considerations:\n",
    "\n",
    "- Handle missing values created by lag features appropriately\n",
    "- Scale or normalize features if needed (though tree-based models like XGBoost and LightGBM can handle different scales)\n",
    "- Consider removing highly correlated features\n",
    "- Use feature importance from initial models to select most relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill for time series data\n",
    "df = df.fillna(method='ffill')\n",
    "\n",
    "# Or drop rows with missing values if at the start of the dataset\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection using correlation:\n",
    "def remove_highly_correlated_features(df, threshold=0.95, method='pearson'):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df.corr(method=method).abs()\n",
    "    \n",
    "    # Create upper triangle matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features to drop\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    print(f\"Features to drop: {to_drop}\")\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "# 사용 예시:\n",
    "df_features = remove_highly_correlated_features(df, threshold=0.95, method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(df, threshold=0.95, method='pearson'):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features and return both cleaned dataframe and list of removed features\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe\n",
    "    threshold (float): Correlation threshold for removal (default: 0.95)\n",
    "    method (str): Correlation method ('pearson', 'spearman', or 'kendall')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (cleaned_df, dropped_features, correlation_details)\n",
    "        - cleaned_df: DataFrame with highly correlated features removed\n",
    "        - dropped_features: List of removed feature names\n",
    "        - correlation_details: DataFrame containing details of dropped correlations\n",
    "    \"\"\"\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = df.corr(method=method).abs()\n",
    "    \n",
    "    # Create upper triangle matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Store correlation details\n",
    "    correlation_details = []\n",
    "    \n",
    "    # Find features to drop\n",
    "    to_drop = []\n",
    "    for column in upper.columns:\n",
    "        # Find highly correlated pairs\n",
    "        high_corr = upper[column][upper[column] > threshold]\n",
    "        \n",
    "        if len(high_corr) > 0:\n",
    "            if column not in to_drop:\n",
    "                to_drop.append(column)\n",
    "                \n",
    "            # Store correlation details\n",
    "            for idx, corr in high_corr.items():\n",
    "                correlation_details.append({\n",
    "                    'dropped_feature': column,\n",
    "                    'correlated_with': idx,\n",
    "                    'correlation': corr\n",
    "                })\n",
    "    \n",
    "    # Convert correlation details to DataFrame\n",
    "    correlation_details = pd.DataFrame(correlation_details)\n",
    "    \n",
    "    # Remove features\n",
    "    cleaned_df = df.drop(columns=to_drop)\n",
    "    \n",
    "    return cleaned_df, to_drop, correlation_details\n",
    "\n",
    "# 사용 예시:\n",
    "cleaned_df, dropped_features, corr_details = remove_highly_correlated_features(df, threshold=0.95, method='pearson')\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\nDropped Features:\", dropped_features)\n",
    "print(\"\\nCorrelation Details:\")\n",
    "print(corr_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_mpae(y_true, y_pred, epsilon=0.5):\n",
    "   \"\"\"\n",
    "   Calculate Mean Percentage Absolute Error with epsilon adjustment for zero values\n",
    "   \"\"\"\n",
    "   y_true = np.array(y_true)\n",
    "   y_pred = np.array(y_pred)\n",
    "   \n",
    "   # Add epsilon to zero values\n",
    "   y_true = np.where(y_true == 0, epsilon, y_true)\n",
    "   \n",
    "   # Calculate percentage absolute error\n",
    "   pae = np.abs((y_true - y_pred) / y_true) * 100\n",
    "   \n",
    "   return np.mean(pae)\n",
    "\n",
    "def train_evaluate_xgboost(X, y, random_state=42):\n",
    "   \"\"\"\n",
    "   Train and evaluate XGBoost model\n",
    "   \"\"\"\n",
    "   # Split data into training and testing sets\n",
    "   X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X, y, test_size=0.2, shuffle=False  # No shuffle for time series data\n",
    "   )\n",
    "   \n",
    "   # Initialize XGBoost model\n",
    "   xgb_model = xgb.XGBRegressor(\n",
    "       n_estimators=1000,\n",
    "       learning_rate=0.01,\n",
    "       max_depth=7,\n",
    "       min_child_weight=1,\n",
    "       subsample=0.8,\n",
    "       colsample_bytree=0.8,\n",
    "       random_state=random_state,\n",
    "       n_jobs=-1\n",
    "   )\n",
    "   \n",
    "   # Train model with early stopping\n",
    "   eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "   xgb_model.fit(\n",
    "       X_train, y_train,\n",
    "       eval_set=eval_set,\n",
    "       eval_metric='rmse',\n",
    "       early_stopping_rounds=50,\n",
    "       verbose=100\n",
    "   )\n",
    "   \n",
    "   # Make predictions\n",
    "   y_pred = xgb_model.predict(X_test)\n",
    "   \n",
    "   # Calculate metrics\n",
    "   metrics = {\n",
    "       'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "       'mae': mean_absolute_error(y_test, y_pred),\n",
    "       'r2': r2_score(y_test, y_pred),\n",
    "       'mpae': calculate_mpae(y_test, y_pred, epsilon=0.5)\n",
    "   }\n",
    "   \n",
    "   # Get feature importance\n",
    "   feature_importance = pd.DataFrame({\n",
    "       'feature': X_train.columns,\n",
    "       'importance': xgb_model.feature_importances_\n",
    "   }).sort_values('importance', ascending=False)\n",
    "   \n",
    "   # Print model performance\n",
    "   print(\"\\n=== Model Performance ===\")\n",
    "   print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "   print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "   print(f\"R2: {metrics['r2']:.4f}\")\n",
    "   print(f\"MPAE: {metrics['mpae']:.4f}%\")\n",
    "   \n",
    "   # Plot feature importance\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   sns.barplot(\n",
    "       data=feature_importance.head(10),\n",
    "       x='importance',\n",
    "       y='feature'\n",
    "   )\n",
    "   plt.title('XGBoost Top 10 Feature Importance')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   # Plot actual vs predicted\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "   plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "   plt.xlabel('Actual')\n",
    "   plt.ylabel('Predicted')\n",
    "   plt.title('XGBoost: Actual vs Predicted')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   results = {\n",
    "       'model': xgb_model,\n",
    "       'predictions': y_pred,\n",
    "       'metrics': metrics,\n",
    "       'feature_importance': feature_importance,\n",
    "       'test_index': X_test.index\n",
    "   }\n",
    "   \n",
    "   return results\n",
    "\n",
    "# 하이퍼파라미터 튜닝을 위한 함수\n",
    "def tune_xgboost(X_train, y_train):\n",
    "   \"\"\"\n",
    "   Tune XGBoost hyperparameters using GridSearchCV\n",
    "   \"\"\"\n",
    "   param_grid = {\n",
    "       'max_depth': [3, 5, 7],\n",
    "       'learning_rate': [0.01, 0.1],\n",
    "       'n_estimators': [100, 500, 1000],\n",
    "       'min_child_weight': [1, 3, 5],\n",
    "       'subsample': [0.6, 0.8, 1.0],\n",
    "       'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "   }\n",
    "   \n",
    "   xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "   grid_search = GridSearchCV(\n",
    "       estimator=xgb_model,\n",
    "       param_grid=param_grid,\n",
    "       cv=TimeSeriesSplit(n_splits=5),\n",
    "       scoring='neg_root_mean_squared_error',\n",
    "       n_jobs=-1,\n",
    "       verbose=2\n",
    "   )\n",
    "   \n",
    "   grid_search.fit(X_train, y_train)\n",
    "   print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "   return grid_search.best_params_\n",
    "\n",
    "# 모델 실행\n",
    "def run_xgboost_modeling(df, target_col, feature_cols):\n",
    "   \"\"\"\n",
    "   Run complete XGBoost modeling process\n",
    "   \"\"\"\n",
    "   # Prepare data\n",
    "   X = df[feature_cols]\n",
    "   y = df[target_col]\n",
    "   \n",
    "   # Train and evaluate model\n",
    "   results = train_evaluate_xgboost(X, y)\n",
    "   \n",
    "   return results\n",
    "\n",
    "# 사용 예시:\n",
    "# feature_cols = [col for col in df.columns if col != target_col]\n",
    "# results = run_xgboost_modeling(df, target_col='actual_gas_demand', feature_cols=feature_cols)\n",
    "\n",
    "# 모델 저장\n",
    "# results['model'].save_model('xgb_model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def calculate_mpae(y_true, y_pred, epsilon=0.5):\n",
    "   \"\"\"\n",
    "   Calculate Mean Percentage Absolute Error with epsilon adjustment for zero values\n",
    "   \"\"\"\n",
    "   y_true = np.array(y_true)\n",
    "   y_pred = np.array(y_pred)\n",
    "   \n",
    "   # Add epsilon to zero values\n",
    "   y_true = np.where(y_true == 0, epsilon, y_true)\n",
    "   \n",
    "   # Calculate percentage absolute error\n",
    "   pae = np.abs((y_true - y_pred) / y_true) * 100\n",
    "   \n",
    "   return np.mean(pae)\n",
    "\n",
    "def train_evaluate_lightgbm(X, y, random_state=42):\n",
    "   \"\"\"\n",
    "   Train and evaluate LightGBM model\n",
    "   \"\"\"\n",
    "   # Split data into training and testing sets\n",
    "   X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X, y, test_size=0.2, shuffle=False  # No shuffle for time series data\n",
    "   )\n",
    "   \n",
    "   # Initialize LightGBM model\n",
    "   lgb_model = lgb.LGBMRegressor(\n",
    "       n_estimators=1000,\n",
    "       learning_rate=0.01,\n",
    "       num_leaves=31,\n",
    "       subsample=0.8,\n",
    "       colsample_bytree=0.8,\n",
    "       random_state=random_state,\n",
    "       n_jobs=-1\n",
    "   )\n",
    "   \n",
    "   # Train model with early stopping\n",
    "   eval_set = [(X_test, y_test)]\n",
    "   lgb_model.fit(\n",
    "       X_train, y_train,\n",
    "       eval_set=eval_set,\n",
    "       eval_metric='rmse',\n",
    "       early_stopping_rounds=50,\n",
    "       verbose=100\n",
    "   )\n",
    "   \n",
    "   # Make predictions\n",
    "   y_pred = lgb_model.predict(X_test)\n",
    "   \n",
    "   # Calculate metrics\n",
    "   metrics = {\n",
    "       'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "       'mae': mean_absolute_error(y_test, y_pred),\n",
    "       'r2': r2_score(y_test, y_pred),\n",
    "       'mpae': calculate_mpae(y_test, y_pred, epsilon=0.5)\n",
    "   }\n",
    "   \n",
    "   # Get feature importance\n",
    "   feature_importance = pd.DataFrame({\n",
    "       'feature': X_train.columns,\n",
    "       'importance': lgb_model.feature_importances_\n",
    "   }).sort_values('importance', ascending=False)\n",
    "   \n",
    "   # Print model performance\n",
    "   print(\"\\n=== Model Performance ===\")\n",
    "   print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "   print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "   print(f\"R2: {metrics['r2']:.4f}\")\n",
    "   print(f\"MPAE: {metrics['mpae']:.4f}%\")\n",
    "   \n",
    "   # Plot feature importance\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   sns.barplot(\n",
    "       data=feature_importance.head(10),\n",
    "       x='importance',\n",
    "       y='feature'\n",
    "   )\n",
    "   plt.title('LightGBM Top 10 Feature Importance')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   # Plot actual vs predicted\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "   plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "   plt.xlabel('Actual')\n",
    "   plt.ylabel('Predicted')\n",
    "   plt.title('LightGBM: Actual vs Predicted')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   \n",
    "   results = {\n",
    "       'model': lgb_model,\n",
    "       'predictions': y_pred,\n",
    "       'metrics': metrics,\n",
    "       'feature_importance': feature_importance,\n",
    "       'test_index': X_test.index\n",
    "   }\n",
    "   \n",
    "   return results\n",
    "\n",
    "# 하이퍼파라미터 튜닝을 위한 함수\n",
    "def tune_lightgbm(X_train, y_train):\n",
    "   \"\"\"\n",
    "   Tune LightGBM hyperparameters using GridSearchCV\n",
    "   \"\"\"\n",
    "   param_grid = {\n",
    "       'num_leaves': [31, 62, 127],\n",
    "       'learning_rate': [0.01, 0.1],\n",
    "       'n_estimators': [100, 500, 1000],\n",
    "       'subsample': [0.6, 0.8, 1.0],\n",
    "       'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "       'min_child_samples': [20, 50, 100]\n",
    "   }\n",
    "   \n",
    "   lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "   grid_search = GridSearchCV(\n",
    "       estimator=lgb_model,\n",
    "       param_grid=param_grid,\n",
    "       cv=TimeSeriesSplit(n_splits=5),\n",
    "       scoring='neg_root_mean_squared_error',\n",
    "       n_jobs=-1,\n",
    "       verbose=2\n",
    "   )\n",
    "   \n",
    "   grid_search.fit(X_train, y_train)\n",
    "   print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "   return grid_search.best_params_\n",
    "\n",
    "# 모델 실행\n",
    "def run_lightgbm_modeling(df, target_col, feature_cols):\n",
    "   \"\"\"\n",
    "   Run complete LightGBM modeling process\n",
    "   \"\"\"\n",
    "   # Prepare data\n",
    "   X = df[feature_cols]\n",
    "   y = df[target_col]\n",
    "   \n",
    "   # Train and evaluate model\n",
    "   results = train_evaluate_lightgbm(X, y)\n",
    "   \n",
    "   return results\n",
    "\n",
    "# 사용 예시:\n",
    "# feature_cols = [col for col in df.columns if col != target_col]\n",
    "# results = run_lightgbm_modeling(df, target_col='actual_gas_demand', feature_cols=feature_cols)\n",
    "\n",
    "# 모델 저장\n",
    "# results['model'].save_model('lgb_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def train_evaluate_models(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate XGBoost and LightGBM models\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature matrix\n",
    "    y (pd.Series): Target variable\n",
    "    random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing trained models and their performance metrics\n",
    "    \"\"\"\n",
    "    # Time series split for validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False  # No shuffle for time series data\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=7,\n",
    "        min_child_weight=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train models with early stopping\n",
    "    eval_set_xgb = [(X_train, y_train), (X_test, y_test)]\n",
    "    eval_set_lgb = [(X_test, y_test)]\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=eval_set_xgb,\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Train LightGBM\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=eval_set_lgb,\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    lgb_pred = lgb_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    def calculate_metrics(y_true, y_pred, model_name):\n",
    "        return {\n",
    "            f'{model_name}_rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            f'{model_name}_mae': mean_absolute_error(y_true, y_pred),\n",
    "            f'{model_name}_r2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    # Get metrics for both models\n",
    "    xgb_metrics = calculate_metrics(y_test, xgb_pred, 'xgb')\n",
    "    lgb_metrics = calculate_metrics(y_test, lgb_pred, 'lgb')\n",
    "    \n",
    "    # Get feature importance\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    lgb_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Combine all results\n",
    "    results = {\n",
    "        'xgb_model': xgb_model,\n",
    "        'lgb_model': lgb_model,\n",
    "        'xgb_predictions': xgb_pred,\n",
    "        'lgb_predictions': lgb_pred,\n",
    "        'metrics': {**xgb_metrics, **lgb_metrics},\n",
    "        'feature_importance': {\n",
    "            'xgb': xgb_importance,\n",
    "            'lgb': lgb_importance\n",
    "        },\n",
    "        'test_index': X_test.index\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 모델 학습 및 평가 실행\n",
    "def run_modeling(df, target_col, feature_cols):\n",
    "    \"\"\"\n",
    "    Run the complete modeling process\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Complete dataset\n",
    "    target_col (str): Name of target column\n",
    "    feature_cols (list): List of feature column names\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = train_evaluate_models(X, y)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== Model Performance ===\")\n",
    "    print(\"\\nXGBoost Metrics:\")\n",
    "    print(f\"RMSE: {results['metrics']['xgb_rmse']:.4f}\")\n",
    "    print(f\"MAE: {results['metrics']['xgb_mae']:.4f}\")\n",
    "    print(f\"R2: {results['metrics']['xgb_r2']:.4f}\")\n",
    "    \n",
    "    print(\"\\nLightGBM Metrics:\")\n",
    "    print(f\"RMSE: {results['metrics']['lgb_rmse']:.4f}\")\n",
    "    print(f\"MAE: {results['metrics']['lgb_mae']:.4f}\")\n",
    "    print(f\"R2: {results['metrics']['lgb_r2']:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(\n",
    "        data=results['feature_importance']['xgb'].head(10),\n",
    "        x='importance',\n",
    "        y='feature'\n",
    "    )\n",
    "    plt.title('XGBoost Top 10 Feature Importance')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(\n",
    "        data=results['feature_importance']['lgb'].head(10),\n",
    "        x='importance',\n",
    "        y='feature'\n",
    "    )\n",
    "    plt.title('LightGBM Top 10 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y[results['test_index']], results['xgb_predictions'], alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('XGBoost: Actual vs Predicted')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y[results['test_index']], results['lgb_predictions'], alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('LightGBM: Actual vs Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 사용 예시:\n",
    "# feature_cols = [col for col in df.columns if col != target_col]\n",
    "# results = run_modeling(df, target_col='actual_gas_demand', feature_cols=feature_cols)\n",
    "\n",
    "# 모델 저장\n",
    "# results['xgb_model'].save_model('xgb_model.json')\n",
    "# results['lgb_model'].save_model('lgb_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_xgboost(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 500, 1000],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=TimeSeriesSplit(n_splits=5),\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
